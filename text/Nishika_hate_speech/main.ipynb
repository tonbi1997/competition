{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの取得、整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\DEV\\\\competition\\\\text\\\\Nishika_hate_speech',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\python38.zip',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\DLLs',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib',\n",
       " 'd:\\\\masa\\\\Anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\masa\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib\\\\site-packages',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib\\\\site-packages\\\\geoplot-0.4.1-py3.8.egg',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib\\\\site-packages\\\\contextily-1.0.0-py3.8.egg',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib\\\\site-packages\\\\mapclassify-2.3.0-py3.8.egg',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib\\\\site-packages\\\\descartes-1.1.0-py3.8.egg',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'd:\\\\masa\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x:x.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80074aa43</th>\n",
       "      <td>news4vip</td>\n",
       "      <td>まともに相手されてない人との関係なんてそんな大事にするものか？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378fea6b</th>\n",
       "      <td>livejupiter</td>\n",
       "      <td>最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c535f5613</th>\n",
       "      <td>livejupiter</td>\n",
       "      <td>日本人として生まれても無能な低学歴って分かったら日本人の権利剥奪して追放すべきやろ甘えるな</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e76638295</th>\n",
       "      <td>livejupiter</td>\n",
       "      <td>よくよく思えば川上は配布にしたらとんでもなく有能だよなガチャから引いたら圧倒的歓喜レベルやで</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51e4036bf</th>\n",
       "      <td>newsplus</td>\n",
       "      <td>押井は原作レイプの専門家だから原作マンガの真意を誤解させることに関してはプロだがそれ以外には何も取り柄がない</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                source  \\\n",
       "id                       \n",
       "80074aa43     news4vip   \n",
       "6378fea6b  livejupiter   \n",
       "c535f5613  livejupiter   \n",
       "e76638295  livejupiter   \n",
       "51e4036bf     newsplus   \n",
       "\n",
       "                                                             text  label  \n",
       "id                                                                        \n",
       "80074aa43                         まともに相手されてない人との関係なんてそんな大事にするものか？      0  \n",
       "6378fea6b                  最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!      0  \n",
       "c535f5613           日本人として生まれても無能な低学歴って分かったら日本人の権利剥奪して追放すべきやろ甘えるな      1  \n",
       "e76638295          よくよく思えば川上は配布にしたらとんでもなく有能だよなガチャから引いたら圧倒的歓喜レベルやで      0  \n",
       "51e4036bf  押井は原作レイプの専門家だから原作マンガの真意を誤解させることに関してはプロだがそれ以外には何も取り柄がない      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipython -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインストール\n",
    "#!python -m pip install --upgrade pip\n",
    "#!pip install -U fugashi ipadic \\\n",
    "#    transformers lime captum\\\n",
    "#    scikit-learn numpy --user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データローラーなど"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4204/4204 [00:00<00:00, 841181.85it/s]\n",
      "100%|██████████| 1052/1052 [00:00<00:00, 1051324.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.features = [\n",
    "            {\n",
    "                'text': row.text,\n",
    "                'label': row.label\n",
    "            } for row in tqdm(df.itertuples(), total=df.shape[0])\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "train_df, eval_df = train_test_split(df, train_size=0.8)\n",
    "#eval_df, test_df = train_test_split(eval_df, train_size=0.5)\n",
    "\n",
    "train_dataset = CustomDataset(train_df)\n",
    "eval_dataset = CustomDataset(eval_df)\n",
    "#test_dataset = CustomDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3957, 1: 247})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "l = train_df.label\n",
    "\n",
    "c = collections.Counter(l)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 993, 1: 59})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "l = eval_df.label\n",
    "\n",
    "c = collections.Counter(l)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\masa/.cache\\huggingface\\hub\\models--cl-tohoku--bert-base-japanese\\snapshots\\5dc6dbba88a42d21da3b71025c109c42462307f2\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\masa/.cache\\huggingface\\hub\\models--cl-tohoku--bert-base-japanese\\snapshots\\5dc6dbba88a42d21da3b71025c109c42462307f2\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\masa/.cache\\huggingface\\hub\\models--cl-tohoku--bert-base-japanese\\snapshots\\5dc6dbba88a42d21da3b71025c109c42462307f2\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\masa/.cache\\huggingface\\hub\\models--cl-tohoku--bert-base-japanese\\snapshots\\5dc6dbba88a42d21da3b71025c109c42462307f2\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class CustomCollator():\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        try:\n",
    "            examples = {\n",
    "                'text': list(map(lambda x: x['text'], examples)),\n",
    "                'label': list(map(lambda x: x['label'], examples))\n",
    "            }\n",
    "        except:\n",
    "            examples = {\n",
    "                'text': list(map(lambda x: x['text'], examples))\n",
    "            }\n",
    "        encodings = self.tokenizer(examples['text'],\n",
    "                                   padding=True,\n",
    "                                   truncation=True,\n",
    "                                   max_length=self.max_length,\n",
    "                                   return_tensors='pt')\n",
    "        if 'label' in examples:\n",
    "            encodings['label'] = torch.tensor(examples['label'])\n",
    "        return encodings\n",
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "custom_collator = CustomCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTモデルの動作デバイス\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FocalLossWithOutOneHot(nn.Module):\n",
    "    def __init__(self, gamma=0, eps=1e-7):\n",
    "        super(FocalLossWithOutOneHot, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logit = F.softmax(input, dim=1)\n",
    "        logit = logit.clamp(self.eps, 1. - self.eps)\n",
    "        logit_ls = torch.log(logit)\n",
    "        loss = F.nll_loss(logit_ls, target, reduction=\"none\")\n",
    "        view = target.size() + (1,)\n",
    "        index = target.view(*view)\n",
    "        loss = loss * (1 - logit.gather(1, index).squeeze(1)) ** self.gamma # focal loss\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5,\n",
    "    #weight=True, size_average=True,\n",
    "    ):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "\n",
    "    def forward(self, target, preds, smooth=1):\n",
    "        #print(preds)\n",
    "        #print(target)\n",
    "        preds = F.one_hot(preds, num_classes=2)\n",
    "        \n",
    "        softmax_fn = nn.Softmax(dim=1)\n",
    "        target = softmax_fn(target)\n",
    "        #target = torch.sigmoid(target) \n",
    "        #print(target)\n",
    "        \n",
    "        #flatten label and preds tensors\n",
    "        preds = preds.reshape(-1)\n",
    "        target = target.reshape(-1)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives \n",
    "        TP = (preds * target).sum()\n",
    "        FP = ((1-target) * preds).sum()\n",
    "        FN = (target * (1-preds)).sum()\n",
    "        Tversky = (TP + smooth)/(TP + self.alpha*FP + self.beta*FN + smooth)\n",
    "        return Tversky "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\masa/.cache\\huggingface\\hub\\models--cl-tohoku--bert-base-japanese\\snapshots\\5dc6dbba88a42d21da3b71025c109c42462307f2\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\masa/.cache\\huggingface\\hub\\models--cl-tohoku--bert-base-japanese\\snapshots\\5dc6dbba88a42d21da3b71025c109c42462307f2\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_categories, loss_function=None):\n",
    "        super().__init__()\n",
    "        self.bert = pretrained_model\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.linear = nn.Linear(self.hidden_size, num_categories)\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                token_type_ids=None,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                label=None):\n",
    "        \n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            position_ids=position_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            output_attentions=output_attentions,\n",
    "                            output_hidden_states=output_hidden_states)\n",
    "        \n",
    "        state = outputs.last_hidden_state[:, 0, :]\n",
    "        state = self.linear(state)\n",
    "        \n",
    "        loss=None\n",
    "        if label is not None and self.loss_function is not None:\n",
    "            loss = self.loss_function(state, label)\n",
    "            \n",
    "        attentions=None\n",
    "        if output_attentions:\n",
    "            attentions=outputs.attentions\n",
    "        \n",
    "        hidden_states=None\n",
    "        if output_hidden_states:\n",
    "            hidden_states=outputs.hidden_states\n",
    "        \n",
    "        return ModelOutput(\n",
    "            logits=state,\n",
    "            loss=loss,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            attentions=attentions,\n",
    "            hidden_states=hidden_states\n",
    "        )\n",
    "\n",
    "categories = [0,1]\n",
    "loss_fct = FocalLossWithOutOneHot()\n",
    "#pretrained_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "pretrained_model = AutoModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "model = CustomNet(pretrained_model, len(categories), loss_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\masa/.cache\\huggingface\\hub\\models--cl-tohoku--bert-base-japanese\\snapshots\\5dc6dbba88a42d21da3b71025c109c42462307f2\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\masa/.cache\\huggingface\\hub\\models--cl-tohoku--bert-base-japanese\\snapshots\\5dc6dbba88a42d21da3b71025c109c42462307f2\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_categories, loss_function=None):\n",
    "        super().__init__()\n",
    "        self.bert = pretrained_model\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.linear = nn.Linear(self.hidden_size, num_categories)\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                token_type_ids=None,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                judgement=None):\n",
    "        \n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            position_ids=position_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            output_attentions=output_attentions,\n",
    "                            output_hidden_states=output_hidden_states)\n",
    "        \n",
    "        state = outputs.last_hidden_state[:, 0, :]\n",
    "        state = self.linear(state)\n",
    "        \n",
    "        loss=None\n",
    "        if judgement is not None and self.loss_function is not None:\n",
    "            loss = self.loss_function(state, judgement)\n",
    "        \n",
    "        attentions=None\n",
    "        if output_attentions:\n",
    "            attentions=outputs.attentions\n",
    "        \n",
    "        hidden_states=None\n",
    "        if output_hidden_states:\n",
    "            hidden_states=outputs.hidden_states\n",
    "        print(state)\n",
    "        return ModelOutput(\n",
    "            logits=state,\n",
    "            loss=loss,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            attentions=attentions,\n",
    "            hidden_states=hidden_states\n",
    "        )\n",
    "\n",
    "categories = [0,1]\n",
    "loss_fct = FocalLossWithOutOneHot()\n",
    "pretrained_model = AutoModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "model = CustomNet(pretrained_model, len(categories), loss_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"output\\model\\prod\\pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNet(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (loss_function): FocalLossWithOutOneHot()\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#BERTの最上位層（１２層目）とその下の層（11層目）をアクティブにする\n",
    "for name, param in model.bert.encoder.layer[11].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.bert.encoder.layer[10].named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "for name,param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#BERTの最上位層（１２層目）とその下の層（11層目）をアクティブにする\n",
    "for name, param in model.bert.encoder.layer[11].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.bert.encoder.layer[10].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\"\"\"\n",
    "#sultan/BioM-ELECTRA-Base-Generator\n",
    "for name,param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#BERTの最上位層（１２層目）とその下の層（11層目）をアクティブにする\n",
    "for name, param in model.bert.encoder.layer[11].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.bert.encoder.layer[10].named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNet(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (loss_function): FocalLossWithOutOneHot()\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tunining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 1e-3)\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from typing import Dict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score\n",
    "\n",
    "def custom_compute_metrics(res: EvalPrediction) -> Dict:\n",
    "    # res.predictions, res.label_idsはnumpyのarray\n",
    "    pred = res.predictions.argmax(axis=1)\n",
    "    target = res.label_ids\n",
    "    precision = precision_score(target, pred, average='macro')\n",
    "    recall = recall_score(target, pred, average='macro')\n",
    "    f1 = f1_score(target, pred, average='macro')\n",
    "    fbeta = fbeta_score(target, pred, beta = 7, average='macro')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'fbeta' : fbeta\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output/model',\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    label_names=['label'],\n",
    "    lr_scheduler_type='polynomial',\n",
    "    metric_for_best_model='fbeta',\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNet(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (loss_function): FocalLossWithOutOneHot()\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=custom_collator,\n",
    "    compute_metrics=custom_compute_metrics,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=20)]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4204\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 26300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f809b1d25b48eeb4c6c839a3586d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1052\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.214, 'learning_rate': 4.9501e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ea4e82bc9a42c59d676327b89f3e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/model\\checkpoint-263\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5282158851623535, 'eval_precision': 0.7729658792650919, 'eval_recall': 0.6704132316042808, 'eval_f1': 0.7078619894684446, 'eval_fbeta': 0.6715836500676845, 'eval_runtime': 4.176, 'eval_samples_per_second': 251.916, 'eval_steps_per_second': 15.805, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./output/model\\checkpoint-263\\tokenizer_config.json\n",
      "Special tokens file saved in ./output/model\\checkpoint-263\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train(ignore_keys_for_eval=['last_hidden_state', 'hidden_states', 'attentions'])\n",
    "trainer.save_model(\"output/model/cl-tohoku-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5429\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83c8f3e42f849f59521cc5df3e85bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40834 [1:05:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [126], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay, confusion_matrix\n\u001b[1;32m----> 5\u001b[0m preds_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlast_hidden_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_states\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattentions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds_output\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m y_valid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjudgement\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\trainer.py:2861\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2858\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2860\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2861\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2862\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPrediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix\n\u001b[0;32m   2863\u001b[0m )\n\u001b[0;32m   2864\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2865\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2866\u001b[0m     speed_metrics(\n\u001b[0;32m   2867\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2871\u001b[0m     )\n\u001b[0;32m   2872\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\trainer.py:2988\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2982\u001b[0m     inputs_host \u001b[39m=\u001b[39m (\n\u001b[0;32m   2983\u001b[0m         inputs_decode\n\u001b[0;32m   2984\u001b[0m         \u001b[39mif\u001b[39;00m inputs_host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2985\u001b[0m         \u001b[39melse\u001b[39;00m nested_concat(inputs_host, inputs_decode, padding_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m   2986\u001b[0m     )\n\u001b[0;32m   2987\u001b[0m \u001b[39mif\u001b[39;00m logits \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2988\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pad_across_processes(logits)\n\u001b[0;32m   2989\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_gather(logits)\n\u001b[0;32m   2990\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\trainer.py:3124\u001b[0m, in \u001b[0;36mTrainer._pad_across_processes\u001b[1;34m(self, tensor, pad_index)\u001b[0m\n\u001b[0;32m   3122\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\n\u001b[0;32m   3123\u001b[0m \u001b[39m# Gather all sizes\u001b[39;00m\n\u001b[1;32m-> 3124\u001b[0m size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(tensor\u001b[39m.\u001b[39;49mshape, device\u001b[39m=\u001b[39;49mtensor\u001b[39m.\u001b[39;49mdevice)[\u001b[39mNone\u001b[39;00m]\n\u001b[0;32m   3125\u001b[0m sizes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_gather(size)\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m   3127\u001b[0m max_size \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(s[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sizes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "preds_output = trainer.predict(eval_dataset, ignore_keys=['loss', 'last_hidden_state', 'hidden_states', 'attentions'])\n",
    "\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_valid = np.array(eval_df[\"judgement\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, categories):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=None)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=categories)\n",
    "    disp.plot(cmap=\"Blues\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_preds, y_valid, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 40834/40834 [00:00<00:00, 870536.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.features = [\n",
    "            {\n",
    "                'feature': row.feature\n",
    "            } for row in tqdm(df.itertuples(), total=df.shape[0])\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "test_df = pd.read_csv(\"test.csv\", index_col=0)\n",
    "test_df[\"feature\"]=test_df[\"abstract\"].where(test_df[\"abstract\"].notna(),test_df[\"title\"])\n",
    "\n",
    "test_dataset = TestDataset(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27145</th>\n",
       "      <td>Estimating the potential effects of COVID-19 pandemic on food commodity prices and nutrition security in Nepal</td>\n",
       "      <td>The objective of the paper is to analyse changes in food commodity prices and estimate the potential effects of food price change on nutrition security in Nepal in the context of COVID-19 contagion control measures. It presents a comparative intra-country observational study design looking at events before and during the pandemic (after implementation of contagion control measures). The study design includes three districts, enabling comparison between diverse agroecological zones and geographical contexts. The methodology consists of primary data collection, modelling and quantitative analysis. The analysis is based on actual school meal food baskets which represent culturally and nutritionally optimised food baskets, developed by the local community and notional typical household food baskets. End May/early June 2020 is the 'Post COVID 19' reference point, the same time period in 2019 i.e. June 2019 is the 'Pre COVID 19' reference point. The study finds substantial increase in food commodity prices across food groups and districts with marked inter-district variation. For school meal basket, all micronutrients show large average declines ranging from 9.5% for zinc to 11% for vitamin A. For household food baskets on average, vitamin-A reduced 37% followed by iron at 19%, reduction in zinc is low due to the high zinc content in whole grain cereals. COVID-19 control measures are likely to have contributed to substantial price inflation over the reference period with potentially damaging effects on nutrition security in Nepal with serious implications for vulnerable populations.Copyright Â© 2020 Cambridge University Press. All rights reserved.</td>\n",
       "      <td>The objective of the paper is to analyse changes in food commodity prices and estimate the potential effects of food price change on nutrition security in Nepal in the context of COVID-19 contagion control measures. It presents a comparative intra-country observational study design looking at events before and during the pandemic (after implementation of contagion control measures). The study design includes three districts, enabling comparison between diverse agroecological zones and geographical contexts. The methodology consists of primary data collection, modelling and quantitative analysis. The analysis is based on actual school meal food baskets which represent culturally and nutritionally optimised food baskets, developed by the local community and notional typical household food baskets. End May/early June 2020 is the 'Post COVID 19' reference point, the same time period in 2019 i.e. June 2019 is the 'Pre COVID 19' reference point. The study finds substantial increase in food commodity prices across food groups and districts with marked inter-district variation. For school meal basket, all micronutrients show large average declines ranging from 9.5% for zinc to 11% for vitamin A. For household food baskets on average, vitamin-A reduced 37% followed by iron at 19%, reduction in zinc is low due to the high zinc content in whole grain cereals. COVID-19 control measures are likely to have contributed to substantial price inflation over the reference period with potentially damaging effects on nutrition security in Nepal with serious implications for vulnerable populations.Copyright Â© 2020 Cambridge University Press. All rights reserved.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27146</th>\n",
       "      <td>Leukoerythroblastic reaction in a patient with COVID-19 infection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leukoerythroblastic reaction in a patient with COVID-19 infection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27147</th>\n",
       "      <td>[15O]-water PET and intraoperative brain mapping: A comparison in the localization of eloquent cortex</td>\n",
       "      <td>[15O]-water PET was performed on 12 patients with structural lesions for localization of the motor (r\\ = 5)  language (receptive and expressive; n = 6)  and visual cortex (h = 1). All these patients underwent interactive image-guided surgery using an infrared digitizer and intraoperative electrical stimulation mapping for motor  sensory  language  and visual cortex location. MRI-PET coregistration was performed using a surface matching approach that integrated functional information with interactive image guidance during the surgical procedure. An awake craniotomy with motor and sensory intraoperative stimulation was performed using a registered bipolar electrode that was tracked on real-time during the surgical procedure. Intraoperative functional findings were displayed and saved on the registered MRI images. The sites of functional PET activation during the performance of motor  visual and language tasks were then compared to the results of intraoperative cortical stimulation in /1 patients and visual evoked potentials in one. The results of the PET activation studies were concordant with the findings of intraoperative stimulation in all cases. During resection of the structural lesions  intraoperative stimulation was continued in the subcortical pathways  and five patients had positive responses on areas not identified by the functional PET. Furthermore  3 patients showed transitory changes in function (speech arrest 1  naming difficulty 1  and motor weakness 1) that were reversible after changing the dissection technique or a brain retractor. [1SO]water PET was reliable in identifying the motor  visual  and language cortex. Language-related rCBF increases were highly distributive  although only part of these activations were subjected to intraoperative stimulation. We conclude that [15O]-water PET can be used for preoperative noninvasive identification of functional cortex and may be useful in neurosurgical preplanning. Intraoperative mapping still remains the main means to avoid neurological damage as it can be performed during the entire surgical procedure to avoid damage to cortex  pathways  and damage secondary to ischemia or edema (brain retraction). ﾂｩ 1997 Forefront Publishing Group</td>\n",
       "      <td>[15O]-water PET was performed on 12 patients with structural lesions for localization of the motor (r\\ = 5)  language (receptive and expressive; n = 6)  and visual cortex (h = 1). All these patients underwent interactive image-guided surgery using an infrared digitizer and intraoperative electrical stimulation mapping for motor  sensory  language  and visual cortex location. MRI-PET coregistration was performed using a surface matching approach that integrated functional information with interactive image guidance during the surgical procedure. An awake craniotomy with motor and sensory intraoperative stimulation was performed using a registered bipolar electrode that was tracked on real-time during the surgical procedure. Intraoperative functional findings were displayed and saved on the registered MRI images. The sites of functional PET activation during the performance of motor  visual and language tasks were then compared to the results of intraoperative cortical stimulation in /1 patients and visual evoked potentials in one. The results of the PET activation studies were concordant with the findings of intraoperative stimulation in all cases. During resection of the structural lesions  intraoperative stimulation was continued in the subcortical pathways  and five patients had positive responses on areas not identified by the functional PET. Furthermore  3 patients showed transitory changes in function (speech arrest 1  naming difficulty 1  and motor weakness 1) that were reversible after changing the dissection technique or a brain retractor. [1SO]water PET was reliable in identifying the motor  visual  and language cortex. Language-related rCBF increases were highly distributive  although only part of these activations were subjected to intraoperative stimulation. We conclude that [15O]-water PET can be used for preoperative noninvasive identification of functional cortex and may be useful in neurosurgical preplanning. Intraoperative mapping still remains the main means to avoid neurological damage as it can be performed during the entire surgical procedure to avoid damage to cortex  pathways  and damage secondary to ischemia or edema (brain retraction). ﾂｩ 1997 Forefront Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27148</th>\n",
       "      <td>Adaptive image segmentation for robust measurement of longitudinal brain tissue change.</td>\n",
       "      <td>We present a method that significantly improves magnetic resonance imaging (MRI) based brain tissue segmentation by modeling the topography of boundaries between tissue compartments. Edge operators are used to identify tissue interfaces and thereby more realistically model tissue label dependencies between adjacent voxels on opposite sides of an interface. When applied to a synthetic MRI template corrupted by additive noise  it provided more consistent tissue labeling across noise levels than two commonly used methods (FAST and SPM5). When applied to longitudinal MRI series it provided lesser variability in individual trajectories of tissue change  suggesting superior ability to discriminate real tissue change from noise. These results suggest that this method may be useful for robust longitudinal brain tissue change estimation.</td>\n",
       "      <td>We present a method that significantly improves magnetic resonance imaging (MRI) based brain tissue segmentation by modeling the topography of boundaries between tissue compartments. Edge operators are used to identify tissue interfaces and thereby more realistically model tissue label dependencies between adjacent voxels on opposite sides of an interface. When applied to a synthetic MRI template corrupted by additive noise  it provided more consistent tissue labeling across noise levels than two commonly used methods (FAST and SPM5). When applied to longitudinal MRI series it provided lesser variability in individual trajectories of tissue change  suggesting superior ability to discriminate real tissue change from noise. These results suggest that this method may be useful for robust longitudinal brain tissue change estimation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27149</th>\n",
       "      <td>Comparison of Epidemiological Variations in COVID-19 Patients Inside and Outside of China-A Meta-Analysis</td>\n",
       "      <td>The objective of this study is to compare the epidemiological variations in COVID-19 patients reported in studies from inside and outside of China. We selected COVID-19 observational studies from eight countries, including, China, Italy, Australia, Canada, Korea, Taiwan, Singapore, and the USA, comprising a total of 13 studies and performed a meta-analysis for age, gender, fatality rate, and clinical symptoms of fever, cough, shortness of breath, and diarrhea. The meta-analysis shows that there are differences in symptoms and other characteristics reported by the patients of COVID-19 inside and outside China. Patients in China have a higher proportion of fever, cough, and shortness of breath as compared to patients outside of China. However, we found the opposite results for the gastrointestinal symptoms such as Diarrhea. Patients outside of China have a significantly higher proportion of Diarrhea as compared to patients within China. We also observed gender disparity among our studies, with the male population being more susceptible than the female population. Moreover, the analysis suggests that the fatality rate in China is relatively lower as compared to the fatality rate in other countries. These findings also suggest that the clinical symptoms of COVID-19 should not be generalized to fever, shortness of breath, and cough only but other symptoms such as diarrhea are also prevalent in patients with COVID-19. Copyright B) 2020 Ahmed, Ali and Hasan.</td>\n",
       "      <td>The objective of this study is to compare the epidemiological variations in COVID-19 patients reported in studies from inside and outside of China. We selected COVID-19 observational studies from eight countries, including, China, Italy, Australia, Canada, Korea, Taiwan, Singapore, and the USA, comprising a total of 13 studies and performed a meta-analysis for age, gender, fatality rate, and clinical symptoms of fever, cough, shortness of breath, and diarrhea. The meta-analysis shows that there are differences in symptoms and other characteristics reported by the patients of COVID-19 inside and outside China. Patients in China have a higher proportion of fever, cough, and shortness of breath as compared to patients outside of China. However, we found the opposite results for the gastrointestinal symptoms such as Diarrhea. Patients outside of China have a significantly higher proportion of Diarrhea as compared to patients within China. We also observed gender disparity among our studies, with the male population being more susceptible than the female population. Moreover, the analysis suggests that the fatality rate in China is relatively lower as compared to the fatality rate in other countries. These findings also suggest that the clinical symptoms of COVID-19 should not be generalized to fever, shortness of breath, and cough only but other symptoms such as diarrhea are also prevalent in patients with COVID-19. Copyright B) 2020 Ahmed, Ali and Hasan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67974</th>\n",
       "      <td>Knowledge, Attitude, and Practices of Healthcare Professionals on COVID-19 and Risk Assessment to Prevent the Epidemic Spread: A Multicenter Cross-Sectional Study from Punjab, Pakistan</td>\n",
       "      <td>In the current outbreak of novel coronavirus (COVID-19), healthcare professionals (HCPs) have a primary role in combating the epidemic threat. HCPs are at high risk of not only contracting the infection but also spreading it unknowingly. It is of utmost importance to evaluate their knowledge, attitudes, and practices (KAP) and the ability to assess the risks associated with the outbreak. A cross-sectional online survey involving physicians, pharmacists, and nurses was conducted. A 39-itemed questionnaire based on the World Health Organization (WHO)COVID-19 risk assessment tool was shared with healthcare professionals in three purposively selected key divisions of Punjab province. Out of 500 healthcare professionals, 385 responded to the survey. The majority (70%) were aged 22-29 years; 144 (37.4%) physicians, 113 (29.4%) nurses, and 128 (33.2%) pharmacists completed the survey. Overall, 94.8% of healthcare professionals scored adequately (&gt;14) for COVID-19-related knowledge; 97.9% displayed an optimistic attitude (&gt;42) and 94.5% had an adequate practice score (&gt;28). Kruskal-Wallis and Jonckheere-Terpstra tests showed significant differences (p &lt; 0.05) in KAP and risk assessment scores among groups; physicians and nurses scored higher as compared to pharmacists. Further research and follow-up investigations on disaster management and risk assessment can help policy-makers better tackle future epidemics.</td>\n",
       "      <td>In the current outbreak of novel coronavirus (COVID-19), healthcare professionals (HCPs) have a primary role in combating the epidemic threat. HCPs are at high risk of not only contracting the infection but also spreading it unknowingly. It is of utmost importance to evaluate their knowledge, attitudes, and practices (KAP) and the ability to assess the risks associated with the outbreak. A cross-sectional online survey involving physicians, pharmacists, and nurses was conducted. A 39-itemed questionnaire based on the World Health Organization (WHO)COVID-19 risk assessment tool was shared with healthcare professionals in three purposively selected key divisions of Punjab province. Out of 500 healthcare professionals, 385 responded to the survey. The majority (70%) were aged 22-29 years; 144 (37.4%) physicians, 113 (29.4%) nurses, and 128 (33.2%) pharmacists completed the survey. Overall, 94.8% of healthcare professionals scored adequately (&gt;14) for COVID-19-related knowledge; 97.9% displayed an optimistic attitude (&gt;42) and 94.5% had an adequate practice score (&gt;28). Kruskal-Wallis and Jonckheere-Terpstra tests showed significant differences (p &lt; 0.05) in KAP and risk assessment scores among groups; physicians and nurses scored higher as compared to pharmacists. Further research and follow-up investigations on disaster management and risk assessment can help policy-makers better tackle future epidemics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67975</th>\n",
       "      <td>Safety and Efficacy of Anti-Il6-Receptor Tocilizumab Use in Severe and Critical Patients Affected by Coronavirus Disease 2019: A Comparative Analysis</td>\n",
       "      <td>BACKGROUND: As the novel SARS-CoV-2 pandemic occurred, no specific treatment was yet available. Inflammatory response secondary to viral infection might be the driver of severe diseases. We report the safety and efficacy (in terms of overall survival and hospital discharge) of the anti-IL6 tocilizumab (TCZ) in subjects with COVID-19. METHODS: This retrospective, single-center analysis included all the patients consecutively admitted to our Hospital with severe or critical COVID-19 who started TCZ treatment from March 13th to April 03rd, 2020. A 1:2 matching to patients not treated with TCZ was performed according to age, sex, severity of disease, P/F, Charlson Comorbidity Index and length of time between symptoms onset and hospital admittance. Descriptive statistics and non-parametric tests to compare the groups were applied. Kaplan Meier probability curves and Cox regression models for survival, hospital discharge and orotracheal intubation were used. RESULTS: Seventy-four patients treated with TCZ were matched with 148 matched controls. They were mainly males (81.5%), Caucasian (82.0%) and with a median age of 59 years. The majority (69.8%) showed critical stage COVID-19 disease. TCZ use was associated with a better overall survival (HR 0.499 [95% CI 0.262-0.952], p=0.035) compared to controls but with a longer hospital stay (HR 1.658 [95% CI 1.088-2.524], p=0.019) mainly due to biochemical, respiratory and infectious adverse events. DISCUSSION: TCZ use resulted potentially effective on COVID-19 in terms of overall survival. Caution is warranted given the potential occurrence of adverse events. FINANCIAL SUPPORT: Some of the tocilizumab doses used in the subjects included in this analysis were provided by the \"Multicenter study on the efficacy and tolerability of tocilizumab in the treatment of patients with COVID-19 pneumonia\" (EudraCT Number: 2020-001110-38) supported by the Italian National Agency for Drugs (AIFA). No specific funding support was planned for study design, data collection and analysis and manuscript writing of this paper. Copyright Â© 2020. Published by Elsevier Ltd.</td>\n",
       "      <td>BACKGROUND: As the novel SARS-CoV-2 pandemic occurred, no specific treatment was yet available. Inflammatory response secondary to viral infection might be the driver of severe diseases. We report the safety and efficacy (in terms of overall survival and hospital discharge) of the anti-IL6 tocilizumab (TCZ) in subjects with COVID-19. METHODS: This retrospective, single-center analysis included all the patients consecutively admitted to our Hospital with severe or critical COVID-19 who started TCZ treatment from March 13th to April 03rd, 2020. A 1:2 matching to patients not treated with TCZ was performed according to age, sex, severity of disease, P/F, Charlson Comorbidity Index and length of time between symptoms onset and hospital admittance. Descriptive statistics and non-parametric tests to compare the groups were applied. Kaplan Meier probability curves and Cox regression models for survival, hospital discharge and orotracheal intubation were used. RESULTS: Seventy-four patients treated with TCZ were matched with 148 matched controls. They were mainly males (81.5%), Caucasian (82.0%) and with a median age of 59 years. The majority (69.8%) showed critical stage COVID-19 disease. TCZ use was associated with a better overall survival (HR 0.499 [95% CI 0.262-0.952], p=0.035) compared to controls but with a longer hospital stay (HR 1.658 [95% CI 1.088-2.524], p=0.019) mainly due to biochemical, respiratory and infectious adverse events. DISCUSSION: TCZ use resulted potentially effective on COVID-19 in terms of overall survival. Caution is warranted given the potential occurrence of adverse events. FINANCIAL SUPPORT: Some of the tocilizumab doses used in the subjects included in this analysis were provided by the \"Multicenter study on the efficacy and tolerability of tocilizumab in the treatment of patients with COVID-19 pneumonia\" (EudraCT Number: 2020-001110-38) supported by the Italian National Agency for Drugs (AIFA). No specific funding support was planned for study design, data collection and analysis and manuscript writing of this paper. Copyright Â© 2020. Published by Elsevier Ltd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67976</th>\n",
       "      <td>Functional imaging of head and neck tumors using positron emission tomography</td>\n",
       "      <td>Positron emission tomography (PET) is an imaging modality that generates in vivo maps of tissue radioactivity originating from a labelled substrate of glucose metabolism: 18-fluorine labelled deoxy-glucose (FDG). This study was undertaken to evaluate PET in the detection of head and neck malignancies  and to determine its effectiveness in diagnosing recurrent cancer in operated or irradiated fields. PET revealed that each biopsy-proven tumour is an area of increased radioactivity. Tumour radioactivity ranged from 130% to 300% above that of the cerebellum  and up to 650% above the contralateral  normal side. By basing the maps on tissue metabolic function  PET proved capable of distinguishing tumour (increased radioactivity) from scar tissue (reduced radioactivity). Its application may facilitate the diagnosis of recurrent tumours amid the fibrosis and distortion of normal architecture in operated  irradiated fields</td>\n",
       "      <td>Positron emission tomography (PET) is an imaging modality that generates in vivo maps of tissue radioactivity originating from a labelled substrate of glucose metabolism: 18-fluorine labelled deoxy-glucose (FDG). This study was undertaken to evaluate PET in the detection of head and neck malignancies  and to determine its effectiveness in diagnosing recurrent cancer in operated or irradiated fields. PET revealed that each biopsy-proven tumour is an area of increased radioactivity. Tumour radioactivity ranged from 130% to 300% above that of the cerebellum  and up to 650% above the contralateral  normal side. By basing the maps on tissue metabolic function  PET proved capable of distinguishing tumour (increased radioactivity) from scar tissue (reduced radioactivity). Its application may facilitate the diagnosis of recurrent tumours amid the fibrosis and distortion of normal architecture in operated  irradiated fields</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67977</th>\n",
       "      <td>Effectiveness of 3D virtual imaging</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Effectiveness of 3D virtual imaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67978</th>\n",
       "      <td>A prospective evaluation of thallium-201 single photon emission computerized tomography for brain tumor burden</td>\n",
       "      <td>PURPOSE: The follow-up of patients with malignant brain tumors after surgery  radiation  and/or chemotherapy has been inadequate for evaluating brain tumor burden using computerized tomography (CT) or magnetic resonance imaging (MRI). Thallium-201 has been shown to concentrate in viable tumor  and Tl-201 single photon emission computerized tomography (SPECT) imaging can identify tumor burden more accurately than CT. METHODS AND MATERIALS: Thirty-one patients with glioblastoma and three patients with low grade astrocytoma were studied with Tl-201 SPECT. Histololgic diagnosis was obtained in all patients by biopsy and all patients had CT scans within 2 weeks of the SPECT study. Seventeen patients were followed with one or more SPECT and CT evaluations. RESULTS: Single photon emission computerized tomography studies  after surgery  radiotherapy  and/or chemotherapy  were more accurate than CT in identifying progression or regression of disease. Twenty-three patients had evidence of disease and 11 patients had no evidence of recurrent disease in the initial Tl-201 SPECT study following therapy. Computerized tomography identified 20 of the 23 patients with disease and 6 of 11 patients with no recurrent disease. Follow-up with Tl-201 SPECT in 17 patients suggested progression of disease in 9 patients  while CT showed progression in only 3 patients. Clinical examinations and repeat CT studies confirmed the accuracy of Tl-201 SPECT images. CONCLUSION: We found Tl-201 SPECT more accurate than CT scans in a prospective evaluation of 34 patients with brain tumor. Follow-up studies with both Tl-201 SPECT and CT imaging in 17 patients demonstrated that SPECT was more reliable than CT in identifying progression  improvement  or no change in brain tumor burden</td>\n",
       "      <td>PURPOSE: The follow-up of patients with malignant brain tumors after surgery  radiation  and/or chemotherapy has been inadequate for evaluating brain tumor burden using computerized tomography (CT) or magnetic resonance imaging (MRI). Thallium-201 has been shown to concentrate in viable tumor  and Tl-201 single photon emission computerized tomography (SPECT) imaging can identify tumor burden more accurately than CT. METHODS AND MATERIALS: Thirty-one patients with glioblastoma and three patients with low grade astrocytoma were studied with Tl-201 SPECT. Histololgic diagnosis was obtained in all patients by biopsy and all patients had CT scans within 2 weeks of the SPECT study. Seventeen patients were followed with one or more SPECT and CT evaluations. RESULTS: Single photon emission computerized tomography studies  after surgery  radiotherapy  and/or chemotherapy  were more accurate than CT in identifying progression or regression of disease. Twenty-three patients had evidence of disease and 11 patients had no evidence of recurrent disease in the initial Tl-201 SPECT study following therapy. Computerized tomography identified 20 of the 23 patients with disease and 6 of 11 patients with no recurrent disease. Follow-up with Tl-201 SPECT in 17 patients suggested progression of disease in 9 patients  while CT showed progression in only 3 patients. Clinical examinations and repeat CT studies confirmed the accuracy of Tl-201 SPECT images. CONCLUSION: We found Tl-201 SPECT more accurate than CT scans in a prospective evaluation of 34 patients with brain tumor. Follow-up studies with both Tl-201 SPECT and CT imaging in 17 patients demonstrated that SPECT was more reliable than CT in identifying progression  improvement  or no change in brain tumor burden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40834 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                          title  \\\n",
       "id                                                                                                                                                                                                \n",
       "27145                                                                            Estimating the potential effects of COVID-19 pandemic on food commodity prices and nutrition security in Nepal   \n",
       "27146                                                                                                                         Leukoerythroblastic reaction in a patient with COVID-19 infection   \n",
       "27147                                                                                     [15O]-water PET and intraoperative brain mapping: A comparison in the localization of eloquent cortex   \n",
       "27148                                                                                                   Adaptive image segmentation for robust measurement of longitudinal brain tissue change.   \n",
       "27149                                                                                 Comparison of Epidemiological Variations in COVID-19 Patients Inside and Outside of China-A Meta-Analysis   \n",
       "...                                                                                                                                                                                         ...   \n",
       "67974  Knowledge, Attitude, and Practices of Healthcare Professionals on COVID-19 and Risk Assessment to Prevent the Epidemic Spread: A Multicenter Cross-Sectional Study from Punjab, Pakistan   \n",
       "67975                                     Safety and Efficacy of Anti-Il6-Receptor Tocilizumab Use in Severe and Critical Patients Affected by Coronavirus Disease 2019: A Comparative Analysis   \n",
       "67976                                                                                                             Functional imaging of head and neck tumors using positron emission tomography   \n",
       "67977                                                                                                                                                       Effectiveness of 3D virtual imaging   \n",
       "67978                                                                            A prospective evaluation of thallium-201 single photon emission computerized tomography for brain tumor burden   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        abstract  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "27145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The objective of the paper is to analyse changes in food commodity prices and estimate the potential effects of food price change on nutrition security in Nepal in the context of COVID-19 contagion control measures. It presents a comparative intra-country observational study design looking at events before and during the pandemic (after implementation of contagion control measures). The study design includes three districts, enabling comparison between diverse agroecological zones and geographical contexts. The methodology consists of primary data collection, modelling and quantitative analysis. The analysis is based on actual school meal food baskets which represent culturally and nutritionally optimised food baskets, developed by the local community and notional typical household food baskets. End May/early June 2020 is the 'Post COVID 19' reference point, the same time period in 2019 i.e. June 2019 is the 'Pre COVID 19' reference point. The study finds substantial increase in food commodity prices across food groups and districts with marked inter-district variation. For school meal basket, all micronutrients show large average declines ranging from 9.5% for zinc to 11% for vitamin A. For household food baskets on average, vitamin-A reduced 37% followed by iron at 19%, reduction in zinc is low due to the high zinc content in whole grain cereals. COVID-19 control measures are likely to have contributed to substantial price inflation over the reference period with potentially damaging effects on nutrition security in Nepal with serious implications for vulnerable populations.Copyright Â© 2020 Cambridge University Press. All rights reserved.   \n",
       "27146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        NaN   \n",
       "27147  [15O]-water PET was performed on 12 patients with structural lesions for localization of the motor (r\\ = 5)  language (receptive and expressive; n = 6)  and visual cortex (h = 1). All these patients underwent interactive image-guided surgery using an infrared digitizer and intraoperative electrical stimulation mapping for motor  sensory  language  and visual cortex location. MRI-PET coregistration was performed using a surface matching approach that integrated functional information with interactive image guidance during the surgical procedure. An awake craniotomy with motor and sensory intraoperative stimulation was performed using a registered bipolar electrode that was tracked on real-time during the surgical procedure. Intraoperative functional findings were displayed and saved on the registered MRI images. The sites of functional PET activation during the performance of motor  visual and language tasks were then compared to the results of intraoperative cortical stimulation in /1 patients and visual evoked potentials in one. The results of the PET activation studies were concordant with the findings of intraoperative stimulation in all cases. During resection of the structural lesions  intraoperative stimulation was continued in the subcortical pathways  and five patients had positive responses on areas not identified by the functional PET. Furthermore  3 patients showed transitory changes in function (speech arrest 1  naming difficulty 1  and motor weakness 1) that were reversible after changing the dissection technique or a brain retractor. [1SO]water PET was reliable in identifying the motor  visual  and language cortex. Language-related rCBF increases were highly distributive  although only part of these activations were subjected to intraoperative stimulation. We conclude that [15O]-water PET can be used for preoperative noninvasive identification of functional cortex and may be useful in neurosurgical preplanning. Intraoperative mapping still remains the main means to avoid neurological damage as it can be performed during the entire surgical procedure to avoid damage to cortex  pathways  and damage secondary to ischemia or edema (brain retraction). ﾂｩ 1997 Forefront Publishing Group   \n",
       "27148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present a method that significantly improves magnetic resonance imaging (MRI) based brain tissue segmentation by modeling the topography of boundaries between tissue compartments. Edge operators are used to identify tissue interfaces and thereby more realistically model tissue label dependencies between adjacent voxels on opposite sides of an interface. When applied to a synthetic MRI template corrupted by additive noise  it provided more consistent tissue labeling across noise levels than two commonly used methods (FAST and SPM5). When applied to longitudinal MRI series it provided lesser variability in individual trajectories of tissue change  suggesting superior ability to discriminate real tissue change from noise. These results suggest that this method may be useful for robust longitudinal brain tissue change estimation.   \n",
       "27149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The objective of this study is to compare the epidemiological variations in COVID-19 patients reported in studies from inside and outside of China. We selected COVID-19 observational studies from eight countries, including, China, Italy, Australia, Canada, Korea, Taiwan, Singapore, and the USA, comprising a total of 13 studies and performed a meta-analysis for age, gender, fatality rate, and clinical symptoms of fever, cough, shortness of breath, and diarrhea. The meta-analysis shows that there are differences in symptoms and other characteristics reported by the patients of COVID-19 inside and outside China. Patients in China have a higher proportion of fever, cough, and shortness of breath as compared to patients outside of China. However, we found the opposite results for the gastrointestinal symptoms such as Diarrhea. Patients outside of China have a significantly higher proportion of Diarrhea as compared to patients within China. We also observed gender disparity among our studies, with the male population being more susceptible than the female population. Moreover, the analysis suggests that the fatality rate in China is relatively lower as compared to the fatality rate in other countries. These findings also suggest that the clinical symptoms of COVID-19 should not be generalized to fever, shortness of breath, and cough only but other symptoms such as diarrhea are also prevalent in patients with COVID-19. Copyright B) 2020 Ahmed, Ali and Hasan.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...   \n",
       "67974                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In the current outbreak of novel coronavirus (COVID-19), healthcare professionals (HCPs) have a primary role in combating the epidemic threat. HCPs are at high risk of not only contracting the infection but also spreading it unknowingly. It is of utmost importance to evaluate their knowledge, attitudes, and practices (KAP) and the ability to assess the risks associated with the outbreak. A cross-sectional online survey involving physicians, pharmacists, and nurses was conducted. A 39-itemed questionnaire based on the World Health Organization (WHO)COVID-19 risk assessment tool was shared with healthcare professionals in three purposively selected key divisions of Punjab province. Out of 500 healthcare professionals, 385 responded to the survey. The majority (70%) were aged 22-29 years; 144 (37.4%) physicians, 113 (29.4%) nurses, and 128 (33.2%) pharmacists completed the survey. Overall, 94.8% of healthcare professionals scored adequately (>14) for COVID-19-related knowledge; 97.9% displayed an optimistic attitude (>42) and 94.5% had an adequate practice score (>28). Kruskal-Wallis and Jonckheere-Terpstra tests showed significant differences (p < 0.05) in KAP and risk assessment scores among groups; physicians and nurses scored higher as compared to pharmacists. Further research and follow-up investigations on disaster management and risk assessment can help policy-makers better tackle future epidemics.   \n",
       "67975                                                                                                               BACKGROUND: As the novel SARS-CoV-2 pandemic occurred, no specific treatment was yet available. Inflammatory response secondary to viral infection might be the driver of severe diseases. We report the safety and efficacy (in terms of overall survival and hospital discharge) of the anti-IL6 tocilizumab (TCZ) in subjects with COVID-19. METHODS: This retrospective, single-center analysis included all the patients consecutively admitted to our Hospital with severe or critical COVID-19 who started TCZ treatment from March 13th to April 03rd, 2020. A 1:2 matching to patients not treated with TCZ was performed according to age, sex, severity of disease, P/F, Charlson Comorbidity Index and length of time between symptoms onset and hospital admittance. Descriptive statistics and non-parametric tests to compare the groups were applied. Kaplan Meier probability curves and Cox regression models for survival, hospital discharge and orotracheal intubation were used. RESULTS: Seventy-four patients treated with TCZ were matched with 148 matched controls. They were mainly males (81.5%), Caucasian (82.0%) and with a median age of 59 years. The majority (69.8%) showed critical stage COVID-19 disease. TCZ use was associated with a better overall survival (HR 0.499 [95% CI 0.262-0.952], p=0.035) compared to controls but with a longer hospital stay (HR 1.658 [95% CI 1.088-2.524], p=0.019) mainly due to biochemical, respiratory and infectious adverse events. DISCUSSION: TCZ use resulted potentially effective on COVID-19 in terms of overall survival. Caution is warranted given the potential occurrence of adverse events. FINANCIAL SUPPORT: Some of the tocilizumab doses used in the subjects included in this analysis were provided by the \"Multicenter study on the efficacy and tolerability of tocilizumab in the treatment of patients with COVID-19 pneumonia\" (EudraCT Number: 2020-001110-38) supported by the Italian National Agency for Drugs (AIFA). No specific funding support was planned for study design, data collection and analysis and manuscript writing of this paper. Copyright Â© 2020. Published by Elsevier Ltd.   \n",
       "67976                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Positron emission tomography (PET) is an imaging modality that generates in vivo maps of tissue radioactivity originating from a labelled substrate of glucose metabolism: 18-fluorine labelled deoxy-glucose (FDG). This study was undertaken to evaluate PET in the detection of head and neck malignancies  and to determine its effectiveness in diagnosing recurrent cancer in operated or irradiated fields. PET revealed that each biopsy-proven tumour is an area of increased radioactivity. Tumour radioactivity ranged from 130% to 300% above that of the cerebellum  and up to 650% above the contralateral  normal side. By basing the maps on tissue metabolic function  PET proved capable of distinguishing tumour (increased radioactivity) from scar tissue (reduced radioactivity). Its application may facilitate the diagnosis of recurrent tumours amid the fibrosis and distortion of normal architecture in operated  irradiated fields   \n",
       "67977                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        NaN   \n",
       "67978                                                                                                                                                                                                                                                                                                                                                                                                                                                                            PURPOSE: The follow-up of patients with malignant brain tumors after surgery  radiation  and/or chemotherapy has been inadequate for evaluating brain tumor burden using computerized tomography (CT) or magnetic resonance imaging (MRI). Thallium-201 has been shown to concentrate in viable tumor  and Tl-201 single photon emission computerized tomography (SPECT) imaging can identify tumor burden more accurately than CT. METHODS AND MATERIALS: Thirty-one patients with glioblastoma and three patients with low grade astrocytoma were studied with Tl-201 SPECT. Histololgic diagnosis was obtained in all patients by biopsy and all patients had CT scans within 2 weeks of the SPECT study. Seventeen patients were followed with one or more SPECT and CT evaluations. RESULTS: Single photon emission computerized tomography studies  after surgery  radiotherapy  and/or chemotherapy  were more accurate than CT in identifying progression or regression of disease. Twenty-three patients had evidence of disease and 11 patients had no evidence of recurrent disease in the initial Tl-201 SPECT study following therapy. Computerized tomography identified 20 of the 23 patients with disease and 6 of 11 patients with no recurrent disease. Follow-up with Tl-201 SPECT in 17 patients suggested progression of disease in 9 patients  while CT showed progression in only 3 patients. Clinical examinations and repeat CT studies confirmed the accuracy of Tl-201 SPECT images. CONCLUSION: We found Tl-201 SPECT more accurate than CT scans in a prospective evaluation of 34 patients with brain tumor. Follow-up studies with both Tl-201 SPECT and CT imaging in 17 patients demonstrated that SPECT was more reliable than CT in identifying progression  improvement  or no change in brain tumor burden   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         feature  \n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "27145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The objective of the paper is to analyse changes in food commodity prices and estimate the potential effects of food price change on nutrition security in Nepal in the context of COVID-19 contagion control measures. It presents a comparative intra-country observational study design looking at events before and during the pandemic (after implementation of contagion control measures). The study design includes three districts, enabling comparison between diverse agroecological zones and geographical contexts. The methodology consists of primary data collection, modelling and quantitative analysis. The analysis is based on actual school meal food baskets which represent culturally and nutritionally optimised food baskets, developed by the local community and notional typical household food baskets. End May/early June 2020 is the 'Post COVID 19' reference point, the same time period in 2019 i.e. June 2019 is the 'Pre COVID 19' reference point. The study finds substantial increase in food commodity prices across food groups and districts with marked inter-district variation. For school meal basket, all micronutrients show large average declines ranging from 9.5% for zinc to 11% for vitamin A. For household food baskets on average, vitamin-A reduced 37% followed by iron at 19%, reduction in zinc is low due to the high zinc content in whole grain cereals. COVID-19 control measures are likely to have contributed to substantial price inflation over the reference period with potentially damaging effects on nutrition security in Nepal with serious implications for vulnerable populations.Copyright Â© 2020 Cambridge University Press. All rights reserved.  \n",
       "27146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Leukoerythroblastic reaction in a patient with COVID-19 infection  \n",
       "27147  [15O]-water PET was performed on 12 patients with structural lesions for localization of the motor (r\\ = 5)  language (receptive and expressive; n = 6)  and visual cortex (h = 1). All these patients underwent interactive image-guided surgery using an infrared digitizer and intraoperative electrical stimulation mapping for motor  sensory  language  and visual cortex location. MRI-PET coregistration was performed using a surface matching approach that integrated functional information with interactive image guidance during the surgical procedure. An awake craniotomy with motor and sensory intraoperative stimulation was performed using a registered bipolar electrode that was tracked on real-time during the surgical procedure. Intraoperative functional findings were displayed and saved on the registered MRI images. The sites of functional PET activation during the performance of motor  visual and language tasks were then compared to the results of intraoperative cortical stimulation in /1 patients and visual evoked potentials in one. The results of the PET activation studies were concordant with the findings of intraoperative stimulation in all cases. During resection of the structural lesions  intraoperative stimulation was continued in the subcortical pathways  and five patients had positive responses on areas not identified by the functional PET. Furthermore  3 patients showed transitory changes in function (speech arrest 1  naming difficulty 1  and motor weakness 1) that were reversible after changing the dissection technique or a brain retractor. [1SO]water PET was reliable in identifying the motor  visual  and language cortex. Language-related rCBF increases were highly distributive  although only part of these activations were subjected to intraoperative stimulation. We conclude that [15O]-water PET can be used for preoperative noninvasive identification of functional cortex and may be useful in neurosurgical preplanning. Intraoperative mapping still remains the main means to avoid neurological damage as it can be performed during the entire surgical procedure to avoid damage to cortex  pathways  and damage secondary to ischemia or edema (brain retraction). ﾂｩ 1997 Forefront Publishing Group  \n",
       "27148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present a method that significantly improves magnetic resonance imaging (MRI) based brain tissue segmentation by modeling the topography of boundaries between tissue compartments. Edge operators are used to identify tissue interfaces and thereby more realistically model tissue label dependencies between adjacent voxels on opposite sides of an interface. When applied to a synthetic MRI template corrupted by additive noise  it provided more consistent tissue labeling across noise levels than two commonly used methods (FAST and SPM5). When applied to longitudinal MRI series it provided lesser variability in individual trajectories of tissue change  suggesting superior ability to discriminate real tissue change from noise. These results suggest that this method may be useful for robust longitudinal brain tissue change estimation.  \n",
       "27149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The objective of this study is to compare the epidemiological variations in COVID-19 patients reported in studies from inside and outside of China. We selected COVID-19 observational studies from eight countries, including, China, Italy, Australia, Canada, Korea, Taiwan, Singapore, and the USA, comprising a total of 13 studies and performed a meta-analysis for age, gender, fatality rate, and clinical symptoms of fever, cough, shortness of breath, and diarrhea. The meta-analysis shows that there are differences in symptoms and other characteristics reported by the patients of COVID-19 inside and outside China. Patients in China have a higher proportion of fever, cough, and shortness of breath as compared to patients outside of China. However, we found the opposite results for the gastrointestinal symptoms such as Diarrhea. Patients outside of China have a significantly higher proportion of Diarrhea as compared to patients within China. We also observed gender disparity among our studies, with the male population being more susceptible than the female population. Moreover, the analysis suggests that the fatality rate in China is relatively lower as compared to the fatality rate in other countries. These findings also suggest that the clinical symptoms of COVID-19 should not be generalized to fever, shortness of breath, and cough only but other symptoms such as diarrhea are also prevalent in patients with COVID-19. Copyright B) 2020 Ahmed, Ali and Hasan.  \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  \n",
       "67974                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In the current outbreak of novel coronavirus (COVID-19), healthcare professionals (HCPs) have a primary role in combating the epidemic threat. HCPs are at high risk of not only contracting the infection but also spreading it unknowingly. It is of utmost importance to evaluate their knowledge, attitudes, and practices (KAP) and the ability to assess the risks associated with the outbreak. A cross-sectional online survey involving physicians, pharmacists, and nurses was conducted. A 39-itemed questionnaire based on the World Health Organization (WHO)COVID-19 risk assessment tool was shared with healthcare professionals in three purposively selected key divisions of Punjab province. Out of 500 healthcare professionals, 385 responded to the survey. The majority (70%) were aged 22-29 years; 144 (37.4%) physicians, 113 (29.4%) nurses, and 128 (33.2%) pharmacists completed the survey. Overall, 94.8% of healthcare professionals scored adequately (>14) for COVID-19-related knowledge; 97.9% displayed an optimistic attitude (>42) and 94.5% had an adequate practice score (>28). Kruskal-Wallis and Jonckheere-Terpstra tests showed significant differences (p < 0.05) in KAP and risk assessment scores among groups; physicians and nurses scored higher as compared to pharmacists. Further research and follow-up investigations on disaster management and risk assessment can help policy-makers better tackle future epidemics.  \n",
       "67975                                                                                                               BACKGROUND: As the novel SARS-CoV-2 pandemic occurred, no specific treatment was yet available. Inflammatory response secondary to viral infection might be the driver of severe diseases. We report the safety and efficacy (in terms of overall survival and hospital discharge) of the anti-IL6 tocilizumab (TCZ) in subjects with COVID-19. METHODS: This retrospective, single-center analysis included all the patients consecutively admitted to our Hospital with severe or critical COVID-19 who started TCZ treatment from March 13th to April 03rd, 2020. A 1:2 matching to patients not treated with TCZ was performed according to age, sex, severity of disease, P/F, Charlson Comorbidity Index and length of time between symptoms onset and hospital admittance. Descriptive statistics and non-parametric tests to compare the groups were applied. Kaplan Meier probability curves and Cox regression models for survival, hospital discharge and orotracheal intubation were used. RESULTS: Seventy-four patients treated with TCZ were matched with 148 matched controls. They were mainly males (81.5%), Caucasian (82.0%) and with a median age of 59 years. The majority (69.8%) showed critical stage COVID-19 disease. TCZ use was associated with a better overall survival (HR 0.499 [95% CI 0.262-0.952], p=0.035) compared to controls but with a longer hospital stay (HR 1.658 [95% CI 1.088-2.524], p=0.019) mainly due to biochemical, respiratory and infectious adverse events. DISCUSSION: TCZ use resulted potentially effective on COVID-19 in terms of overall survival. Caution is warranted given the potential occurrence of adverse events. FINANCIAL SUPPORT: Some of the tocilizumab doses used in the subjects included in this analysis were provided by the \"Multicenter study on the efficacy and tolerability of tocilizumab in the treatment of patients with COVID-19 pneumonia\" (EudraCT Number: 2020-001110-38) supported by the Italian National Agency for Drugs (AIFA). No specific funding support was planned for study design, data collection and analysis and manuscript writing of this paper. Copyright Â© 2020. Published by Elsevier Ltd.  \n",
       "67976                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Positron emission tomography (PET) is an imaging modality that generates in vivo maps of tissue radioactivity originating from a labelled substrate of glucose metabolism: 18-fluorine labelled deoxy-glucose (FDG). This study was undertaken to evaluate PET in the detection of head and neck malignancies  and to determine its effectiveness in diagnosing recurrent cancer in operated or irradiated fields. PET revealed that each biopsy-proven tumour is an area of increased radioactivity. Tumour radioactivity ranged from 130% to 300% above that of the cerebellum  and up to 650% above the contralateral  normal side. By basing the maps on tissue metabolic function  PET proved capable of distinguishing tumour (increased radioactivity) from scar tissue (reduced radioactivity). Its application may facilitate the diagnosis of recurrent tumours amid the fibrosis and distortion of normal architecture in operated  irradiated fields  \n",
       "67977                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Effectiveness of 3D virtual imaging  \n",
       "67978                                                                                                                                                                                                                                                                                                                                                                                                                                                                            PURPOSE: The follow-up of patients with malignant brain tumors after surgery  radiation  and/or chemotherapy has been inadequate for evaluating brain tumor burden using computerized tomography (CT) or magnetic resonance imaging (MRI). Thallium-201 has been shown to concentrate in viable tumor  and Tl-201 single photon emission computerized tomography (SPECT) imaging can identify tumor burden more accurately than CT. METHODS AND MATERIALS: Thirty-one patients with glioblastoma and three patients with low grade astrocytoma were studied with Tl-201 SPECT. Histololgic diagnosis was obtained in all patients by biopsy and all patients had CT scans within 2 weeks of the SPECT study. Seventeen patients were followed with one or more SPECT and CT evaluations. RESULTS: Single photon emission computerized tomography studies  after surgery  radiotherapy  and/or chemotherapy  were more accurate than CT in identifying progression or regression of disease. Twenty-three patients had evidence of disease and 11 patients had no evidence of recurrent disease in the initial Tl-201 SPECT study following therapy. Computerized tomography identified 20 of the 23 patients with disease and 6 of 11 patients with no recurrent disease. Follow-up with Tl-201 SPECT in 17 patients suggested progression of disease in 9 patients  while CT showed progression in only 3 patients. Clinical examinations and repeat CT studies confirmed the accuracy of Tl-201 SPECT images. CONCLUSION: We found Tl-201 SPECT more accurate than CT scans in a prospective evaluation of 34 patients with brain tumor. Follow-up studies with both Tl-201 SPECT and CT imaging in 17 patients demonstrated that SPECT was more reliable than CT in identifying progression  improvement  or no change in brain tumor burden  \n",
       "\n",
       "[40834 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 40834\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "result = trainer.predict(test_dataset, ignore_keys=['loss', 'last_hidden_state', 'hidden_states', 'attentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"label\"] = result.predictions.argmax(axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>feature</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27145</th>\n",
       "      <td>Estimating the potential effects of COVID-19 pandemic on food commodity prices and nutrition security in Nepal</td>\n",
       "      <td>The objective of the paper is to analyse changes in food commodity prices and estimate the potential effects of food price change on nutrition security in Nepal in the context of COVID-19 contagion control measures. It presents a comparative intra-country observational study design looking at events before and during the pandemic (after implementation of contagion control measures). The study design includes three districts, enabling comparison between diverse agroecological zones and geographical contexts. The methodology consists of primary data collection, modelling and quantitative analysis. The analysis is based on actual school meal food baskets which represent culturally and nutritionally optimised food baskets, developed by the local community and notional typical household food baskets. End May/early June 2020 is the 'Post COVID 19' reference point, the same time period in 2019 i.e. June 2019 is the 'Pre COVID 19' reference point. The study finds substantial increase in food commodity prices across food groups and districts with marked inter-district variation. For school meal basket, all micronutrients show large average declines ranging from 9.5% for zinc to 11% for vitamin A. For household food baskets on average, vitamin-A reduced 37% followed by iron at 19%, reduction in zinc is low due to the high zinc content in whole grain cereals. COVID-19 control measures are likely to have contributed to substantial price inflation over the reference period with potentially damaging effects on nutrition security in Nepal with serious implications for vulnerable populations.Copyright Â© 2020 Cambridge University Press. All rights reserved.</td>\n",
       "      <td>The objective of the paper is to analyse changes in food commodity prices and estimate the potential effects of food price change on nutrition security in Nepal in the context of COVID-19 contagion control measures. It presents a comparative intra-country observational study design looking at events before and during the pandemic (after implementation of contagion control measures). The study design includes three districts, enabling comparison between diverse agroecological zones and geographical contexts. The methodology consists of primary data collection, modelling and quantitative analysis. The analysis is based on actual school meal food baskets which represent culturally and nutritionally optimised food baskets, developed by the local community and notional typical household food baskets. End May/early June 2020 is the 'Post COVID 19' reference point, the same time period in 2019 i.e. June 2019 is the 'Pre COVID 19' reference point. The study finds substantial increase in food commodity prices across food groups and districts with marked inter-district variation. For school meal basket, all micronutrients show large average declines ranging from 9.5% for zinc to 11% for vitamin A. For household food baskets on average, vitamin-A reduced 37% followed by iron at 19%, reduction in zinc is low due to the high zinc content in whole grain cereals. COVID-19 control measures are likely to have contributed to substantial price inflation over the reference period with potentially damaging effects on nutrition security in Nepal with serious implications for vulnerable populations.Copyright Â© 2020 Cambridge University Press. All rights reserved.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27146</th>\n",
       "      <td>Leukoerythroblastic reaction in a patient with COVID-19 infection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leukoerythroblastic reaction in a patient with COVID-19 infection</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27147</th>\n",
       "      <td>[15O]-water PET and intraoperative brain mapping: A comparison in the localization of eloquent cortex</td>\n",
       "      <td>[15O]-water PET was performed on 12 patients with structural lesions for localization of the motor (r\\ = 5)  language (receptive and expressive; n = 6)  and visual cortex (h = 1). All these patients underwent interactive image-guided surgery using an infrared digitizer and intraoperative electrical stimulation mapping for motor  sensory  language  and visual cortex location. MRI-PET coregistration was performed using a surface matching approach that integrated functional information with interactive image guidance during the surgical procedure. An awake craniotomy with motor and sensory intraoperative stimulation was performed using a registered bipolar electrode that was tracked on real-time during the surgical procedure. Intraoperative functional findings were displayed and saved on the registered MRI images. The sites of functional PET activation during the performance of motor  visual and language tasks were then compared to the results of intraoperative cortical stimulation in /1 patients and visual evoked potentials in one. The results of the PET activation studies were concordant with the findings of intraoperative stimulation in all cases. During resection of the structural lesions  intraoperative stimulation was continued in the subcortical pathways  and five patients had positive responses on areas not identified by the functional PET. Furthermore  3 patients showed transitory changes in function (speech arrest 1  naming difficulty 1  and motor weakness 1) that were reversible after changing the dissection technique or a brain retractor. [1SO]water PET was reliable in identifying the motor  visual  and language cortex. Language-related rCBF increases were highly distributive  although only part of these activations were subjected to intraoperative stimulation. We conclude that [15O]-water PET can be used for preoperative noninvasive identification of functional cortex and may be useful in neurosurgical preplanning. Intraoperative mapping still remains the main means to avoid neurological damage as it can be performed during the entire surgical procedure to avoid damage to cortex  pathways  and damage secondary to ischemia or edema (brain retraction). ﾂｩ 1997 Forefront Publishing Group</td>\n",
       "      <td>[15O]-water PET was performed on 12 patients with structural lesions for localization of the motor (r\\ = 5)  language (receptive and expressive; n = 6)  and visual cortex (h = 1). All these patients underwent interactive image-guided surgery using an infrared digitizer and intraoperative electrical stimulation mapping for motor  sensory  language  and visual cortex location. MRI-PET coregistration was performed using a surface matching approach that integrated functional information with interactive image guidance during the surgical procedure. An awake craniotomy with motor and sensory intraoperative stimulation was performed using a registered bipolar electrode that was tracked on real-time during the surgical procedure. Intraoperative functional findings were displayed and saved on the registered MRI images. The sites of functional PET activation during the performance of motor  visual and language tasks were then compared to the results of intraoperative cortical stimulation in /1 patients and visual evoked potentials in one. The results of the PET activation studies were concordant with the findings of intraoperative stimulation in all cases. During resection of the structural lesions  intraoperative stimulation was continued in the subcortical pathways  and five patients had positive responses on areas not identified by the functional PET. Furthermore  3 patients showed transitory changes in function (speech arrest 1  naming difficulty 1  and motor weakness 1) that were reversible after changing the dissection technique or a brain retractor. [1SO]water PET was reliable in identifying the motor  visual  and language cortex. Language-related rCBF increases were highly distributive  although only part of these activations were subjected to intraoperative stimulation. We conclude that [15O]-water PET can be used for preoperative noninvasive identification of functional cortex and may be useful in neurosurgical preplanning. Intraoperative mapping still remains the main means to avoid neurological damage as it can be performed during the entire surgical procedure to avoid damage to cortex  pathways  and damage secondary to ischemia or edema (brain retraction). ﾂｩ 1997 Forefront Publishing Group</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27148</th>\n",
       "      <td>Adaptive image segmentation for robust measurement of longitudinal brain tissue change.</td>\n",
       "      <td>We present a method that significantly improves magnetic resonance imaging (MRI) based brain tissue segmentation by modeling the topography of boundaries between tissue compartments. Edge operators are used to identify tissue interfaces and thereby more realistically model tissue label dependencies between adjacent voxels on opposite sides of an interface. When applied to a synthetic MRI template corrupted by additive noise  it provided more consistent tissue labeling across noise levels than two commonly used methods (FAST and SPM5). When applied to longitudinal MRI series it provided lesser variability in individual trajectories of tissue change  suggesting superior ability to discriminate real tissue change from noise. These results suggest that this method may be useful for robust longitudinal brain tissue change estimation.</td>\n",
       "      <td>We present a method that significantly improves magnetic resonance imaging (MRI) based brain tissue segmentation by modeling the topography of boundaries between tissue compartments. Edge operators are used to identify tissue interfaces and thereby more realistically model tissue label dependencies between adjacent voxels on opposite sides of an interface. When applied to a synthetic MRI template corrupted by additive noise  it provided more consistent tissue labeling across noise levels than two commonly used methods (FAST and SPM5). When applied to longitudinal MRI series it provided lesser variability in individual trajectories of tissue change  suggesting superior ability to discriminate real tissue change from noise. These results suggest that this method may be useful for robust longitudinal brain tissue change estimation.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27149</th>\n",
       "      <td>Comparison of Epidemiological Variations in COVID-19 Patients Inside and Outside of China-A Meta-Analysis</td>\n",
       "      <td>The objective of this study is to compare the epidemiological variations in COVID-19 patients reported in studies from inside and outside of China. We selected COVID-19 observational studies from eight countries, including, China, Italy, Australia, Canada, Korea, Taiwan, Singapore, and the USA, comprising a total of 13 studies and performed a meta-analysis for age, gender, fatality rate, and clinical symptoms of fever, cough, shortness of breath, and diarrhea. The meta-analysis shows that there are differences in symptoms and other characteristics reported by the patients of COVID-19 inside and outside China. Patients in China have a higher proportion of fever, cough, and shortness of breath as compared to patients outside of China. However, we found the opposite results for the gastrointestinal symptoms such as Diarrhea. Patients outside of China have a significantly higher proportion of Diarrhea as compared to patients within China. We also observed gender disparity among our studies, with the male population being more susceptible than the female population. Moreover, the analysis suggests that the fatality rate in China is relatively lower as compared to the fatality rate in other countries. These findings also suggest that the clinical symptoms of COVID-19 should not be generalized to fever, shortness of breath, and cough only but other symptoms such as diarrhea are also prevalent in patients with COVID-19. Copyright B) 2020 Ahmed, Ali and Hasan.</td>\n",
       "      <td>The objective of this study is to compare the epidemiological variations in COVID-19 patients reported in studies from inside and outside of China. We selected COVID-19 observational studies from eight countries, including, China, Italy, Australia, Canada, Korea, Taiwan, Singapore, and the USA, comprising a total of 13 studies and performed a meta-analysis for age, gender, fatality rate, and clinical symptoms of fever, cough, shortness of breath, and diarrhea. The meta-analysis shows that there are differences in symptoms and other characteristics reported by the patients of COVID-19 inside and outside China. Patients in China have a higher proportion of fever, cough, and shortness of breath as compared to patients outside of China. However, we found the opposite results for the gastrointestinal symptoms such as Diarrhea. Patients outside of China have a significantly higher proportion of Diarrhea as compared to patients within China. We also observed gender disparity among our studies, with the male population being more susceptible than the female population. Moreover, the analysis suggests that the fatality rate in China is relatively lower as compared to the fatality rate in other countries. These findings also suggest that the clinical symptoms of COVID-19 should not be generalized to fever, shortness of breath, and cough only but other symptoms such as diarrhea are also prevalent in patients with COVID-19. Copyright B) 2020 Ahmed, Ali and Hasan.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                title  \\\n",
       "id                                                                                                                      \n",
       "27145  Estimating the potential effects of COVID-19 pandemic on food commodity prices and nutrition security in Nepal   \n",
       "27146                                               Leukoerythroblastic reaction in a patient with COVID-19 infection   \n",
       "27147           [15O]-water PET and intraoperative brain mapping: A comparison in the localization of eloquent cortex   \n",
       "27148                         Adaptive image segmentation for robust measurement of longitudinal brain tissue change.   \n",
       "27149       Comparison of Epidemiological Variations in COVID-19 Patients Inside and Outside of China-A Meta-Analysis   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        abstract  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "27145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The objective of the paper is to analyse changes in food commodity prices and estimate the potential effects of food price change on nutrition security in Nepal in the context of COVID-19 contagion control measures. It presents a comparative intra-country observational study design looking at events before and during the pandemic (after implementation of contagion control measures). The study design includes three districts, enabling comparison between diverse agroecological zones and geographical contexts. The methodology consists of primary data collection, modelling and quantitative analysis. The analysis is based on actual school meal food baskets which represent culturally and nutritionally optimised food baskets, developed by the local community and notional typical household food baskets. End May/early June 2020 is the 'Post COVID 19' reference point, the same time period in 2019 i.e. June 2019 is the 'Pre COVID 19' reference point. The study finds substantial increase in food commodity prices across food groups and districts with marked inter-district variation. For school meal basket, all micronutrients show large average declines ranging from 9.5% for zinc to 11% for vitamin A. For household food baskets on average, vitamin-A reduced 37% followed by iron at 19%, reduction in zinc is low due to the high zinc content in whole grain cereals. COVID-19 control measures are likely to have contributed to substantial price inflation over the reference period with potentially damaging effects on nutrition security in Nepal with serious implications for vulnerable populations.Copyright Â© 2020 Cambridge University Press. All rights reserved.   \n",
       "27146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        NaN   \n",
       "27147  [15O]-water PET was performed on 12 patients with structural lesions for localization of the motor (r\\ = 5)  language (receptive and expressive; n = 6)  and visual cortex (h = 1). All these patients underwent interactive image-guided surgery using an infrared digitizer and intraoperative electrical stimulation mapping for motor  sensory  language  and visual cortex location. MRI-PET coregistration was performed using a surface matching approach that integrated functional information with interactive image guidance during the surgical procedure. An awake craniotomy with motor and sensory intraoperative stimulation was performed using a registered bipolar electrode that was tracked on real-time during the surgical procedure. Intraoperative functional findings were displayed and saved on the registered MRI images. The sites of functional PET activation during the performance of motor  visual and language tasks were then compared to the results of intraoperative cortical stimulation in /1 patients and visual evoked potentials in one. The results of the PET activation studies were concordant with the findings of intraoperative stimulation in all cases. During resection of the structural lesions  intraoperative stimulation was continued in the subcortical pathways  and five patients had positive responses on areas not identified by the functional PET. Furthermore  3 patients showed transitory changes in function (speech arrest 1  naming difficulty 1  and motor weakness 1) that were reversible after changing the dissection technique or a brain retractor. [1SO]water PET was reliable in identifying the motor  visual  and language cortex. Language-related rCBF increases were highly distributive  although only part of these activations were subjected to intraoperative stimulation. We conclude that [15O]-water PET can be used for preoperative noninvasive identification of functional cortex and may be useful in neurosurgical preplanning. Intraoperative mapping still remains the main means to avoid neurological damage as it can be performed during the entire surgical procedure to avoid damage to cortex  pathways  and damage secondary to ischemia or edema (brain retraction). ﾂｩ 1997 Forefront Publishing Group   \n",
       "27148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present a method that significantly improves magnetic resonance imaging (MRI) based brain tissue segmentation by modeling the topography of boundaries between tissue compartments. Edge operators are used to identify tissue interfaces and thereby more realistically model tissue label dependencies between adjacent voxels on opposite sides of an interface. When applied to a synthetic MRI template corrupted by additive noise  it provided more consistent tissue labeling across noise levels than two commonly used methods (FAST and SPM5). When applied to longitudinal MRI series it provided lesser variability in individual trajectories of tissue change  suggesting superior ability to discriminate real tissue change from noise. These results suggest that this method may be useful for robust longitudinal brain tissue change estimation.   \n",
       "27149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The objective of this study is to compare the epidemiological variations in COVID-19 patients reported in studies from inside and outside of China. We selected COVID-19 observational studies from eight countries, including, China, Italy, Australia, Canada, Korea, Taiwan, Singapore, and the USA, comprising a total of 13 studies and performed a meta-analysis for age, gender, fatality rate, and clinical symptoms of fever, cough, shortness of breath, and diarrhea. The meta-analysis shows that there are differences in symptoms and other characteristics reported by the patients of COVID-19 inside and outside China. Patients in China have a higher proportion of fever, cough, and shortness of breath as compared to patients outside of China. However, we found the opposite results for the gastrointestinal symptoms such as Diarrhea. Patients outside of China have a significantly higher proportion of Diarrhea as compared to patients within China. We also observed gender disparity among our studies, with the male population being more susceptible than the female population. Moreover, the analysis suggests that the fatality rate in China is relatively lower as compared to the fatality rate in other countries. These findings also suggest that the clinical symptoms of COVID-19 should not be generalized to fever, shortness of breath, and cough only but other symptoms such as diarrhea are also prevalent in patients with COVID-19. Copyright B) 2020 Ahmed, Ali and Hasan.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         feature  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "27145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The objective of the paper is to analyse changes in food commodity prices and estimate the potential effects of food price change on nutrition security in Nepal in the context of COVID-19 contagion control measures. It presents a comparative intra-country observational study design looking at events before and during the pandemic (after implementation of contagion control measures). The study design includes three districts, enabling comparison between diverse agroecological zones and geographical contexts. The methodology consists of primary data collection, modelling and quantitative analysis. The analysis is based on actual school meal food baskets which represent culturally and nutritionally optimised food baskets, developed by the local community and notional typical household food baskets. End May/early June 2020 is the 'Post COVID 19' reference point, the same time period in 2019 i.e. June 2019 is the 'Pre COVID 19' reference point. The study finds substantial increase in food commodity prices across food groups and districts with marked inter-district variation. For school meal basket, all micronutrients show large average declines ranging from 9.5% for zinc to 11% for vitamin A. For household food baskets on average, vitamin-A reduced 37% followed by iron at 19%, reduction in zinc is low due to the high zinc content in whole grain cereals. COVID-19 control measures are likely to have contributed to substantial price inflation over the reference period with potentially damaging effects on nutrition security in Nepal with serious implications for vulnerable populations.Copyright Â© 2020 Cambridge University Press. All rights reserved.   \n",
       "27146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Leukoerythroblastic reaction in a patient with COVID-19 infection   \n",
       "27147  [15O]-water PET was performed on 12 patients with structural lesions for localization of the motor (r\\ = 5)  language (receptive and expressive; n = 6)  and visual cortex (h = 1). All these patients underwent interactive image-guided surgery using an infrared digitizer and intraoperative electrical stimulation mapping for motor  sensory  language  and visual cortex location. MRI-PET coregistration was performed using a surface matching approach that integrated functional information with interactive image guidance during the surgical procedure. An awake craniotomy with motor and sensory intraoperative stimulation was performed using a registered bipolar electrode that was tracked on real-time during the surgical procedure. Intraoperative functional findings were displayed and saved on the registered MRI images. The sites of functional PET activation during the performance of motor  visual and language tasks were then compared to the results of intraoperative cortical stimulation in /1 patients and visual evoked potentials in one. The results of the PET activation studies were concordant with the findings of intraoperative stimulation in all cases. During resection of the structural lesions  intraoperative stimulation was continued in the subcortical pathways  and five patients had positive responses on areas not identified by the functional PET. Furthermore  3 patients showed transitory changes in function (speech arrest 1  naming difficulty 1  and motor weakness 1) that were reversible after changing the dissection technique or a brain retractor. [1SO]water PET was reliable in identifying the motor  visual  and language cortex. Language-related rCBF increases were highly distributive  although only part of these activations were subjected to intraoperative stimulation. We conclude that [15O]-water PET can be used for preoperative noninvasive identification of functional cortex and may be useful in neurosurgical preplanning. Intraoperative mapping still remains the main means to avoid neurological damage as it can be performed during the entire surgical procedure to avoid damage to cortex  pathways  and damage secondary to ischemia or edema (brain retraction). ﾂｩ 1997 Forefront Publishing Group   \n",
       "27148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present a method that significantly improves magnetic resonance imaging (MRI) based brain tissue segmentation by modeling the topography of boundaries between tissue compartments. Edge operators are used to identify tissue interfaces and thereby more realistically model tissue label dependencies between adjacent voxels on opposite sides of an interface. When applied to a synthetic MRI template corrupted by additive noise  it provided more consistent tissue labeling across noise levels than two commonly used methods (FAST and SPM5). When applied to longitudinal MRI series it provided lesser variability in individual trajectories of tissue change  suggesting superior ability to discriminate real tissue change from noise. These results suggest that this method may be useful for robust longitudinal brain tissue change estimation.   \n",
       "27149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The objective of this study is to compare the epidemiological variations in COVID-19 patients reported in studies from inside and outside of China. We selected COVID-19 observational studies from eight countries, including, China, Italy, Australia, Canada, Korea, Taiwan, Singapore, and the USA, comprising a total of 13 studies and performed a meta-analysis for age, gender, fatality rate, and clinical symptoms of fever, cough, shortness of breath, and diarrhea. The meta-analysis shows that there are differences in symptoms and other characteristics reported by the patients of COVID-19 inside and outside China. Patients in China have a higher proportion of fever, cough, and shortness of breath as compared to patients outside of China. However, we found the opposite results for the gastrointestinal symptoms such as Diarrhea. Patients outside of China have a significantly higher proportion of Diarrhea as compared to patients within China. We also observed gender disparity among our studies, with the male population being more susceptible than the female population. Moreover, the analysis suggests that the fatality rate in China is relatively lower as compared to the fatality rate in other countries. These findings also suggest that the clinical symptoms of COVID-19 should not be generalized to fever, shortness of breath, and cough only but other symptoms such as diarrhea are also prevalent in patients with COVID-19. Copyright B) 2020 Ahmed, Ali and Hasan.   \n",
       "\n",
       "       label  \n",
       "id            \n",
       "27145      0  \n",
       "27146      0  \n",
       "27147      0  \n",
       "27148      0  \n",
       "27149      0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 38850, 1: 1984})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "l = test_df[\"label\"] \n",
    "\n",
    "c = collections.Counter(l)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test_df=test_df.drop('title', axis=1)\n",
    "#test_df=test_df.drop('abstract', axis=1)\n",
    "test_df=test_df.drop('feature', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('submit_SapBERT.csv', header=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40834"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.predictions.argmax(axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.predictions.argmax(axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [81], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m softmax_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m softmax_output \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\masa\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32md:\\masa\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1198\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1198\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msoftmax(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim, _stacklevel\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32md:\\masa\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1512\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1510\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1511\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1512\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1513\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1514\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "softmax_fn = nn.Softmax(dim=1)\n",
    "softmax_output = softmax_fn(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output/model/prod\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output/model/prod\\tokenizer_config.json\n",
      "Special tokens file saved in output/model/prod\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"output/model/prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubmitDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.features = [\n",
    "            {\n",
    "                'abstract': row.abstract\n",
    "            } for row in tqdm(df.itertuples(), total=df.shape[0])\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 40834/40834 [00:00<00:00, 314942.63it/s]\n"
     ]
    }
   ],
   "source": [
    "submit_df = pd.read_csv(\"test.csv\", index_col=0)\n",
    "submit_df[\"abstract\"]=submit_df[\"abstract\"].where(submit_df[\"abstract\"].notna(),submit_df[\"title\"])\n",
    "submit_df['judgement'] = [0] * len(submit_df)\n",
    "submit_dataset = CustomDataset(submit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df[\"abstract\"]=submit_df[\"abstract\"].where(submit_df[\"abstract\"].notna(),submit_df[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>judgement</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27145</th>\n",
       "      <td>Estimating the potential effects of COVID-19 p...</td>\n",
       "      <td>The objective of the paper is to analyse chang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27146</th>\n",
       "      <td>Leukoerythroblastic reaction in a patient with...</td>\n",
       "      <td>Leukoerythroblastic reaction in a patient with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27147</th>\n",
       "      <td>[15O]-water PET and intraoperative brain mappi...</td>\n",
       "      <td>[15O]-water PET was performed on 12 patients w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27148</th>\n",
       "      <td>Adaptive image segmentation for robust measure...</td>\n",
       "      <td>We present a method that significantly improve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27149</th>\n",
       "      <td>Comparison of Epidemiological Variations in CO...</td>\n",
       "      <td>The objective of this study is to compare the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67974</th>\n",
       "      <td>Knowledge, Attitude, and Practices of Healthca...</td>\n",
       "      <td>In the current outbreak of novel coronavirus (...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67975</th>\n",
       "      <td>Safety and Efficacy of Anti-Il6-Receptor Tocil...</td>\n",
       "      <td>BACKGROUND: As the novel SARS-CoV-2 pandemic o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67976</th>\n",
       "      <td>Functional imaging of head and neck tumors usi...</td>\n",
       "      <td>Positron emission tomography (PET) is an imagi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67977</th>\n",
       "      <td>Effectiveness of 3D virtual imaging</td>\n",
       "      <td>Effectiveness of 3D virtual imaging</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67978</th>\n",
       "      <td>A prospective evaluation of thallium-201 singl...</td>\n",
       "      <td>PURPOSE: The follow-up of patients with malign...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40834 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "id                                                         \n",
       "27145  Estimating the potential effects of COVID-19 p...   \n",
       "27146  Leukoerythroblastic reaction in a patient with...   \n",
       "27147  [15O]-water PET and intraoperative brain mappi...   \n",
       "27148  Adaptive image segmentation for robust measure...   \n",
       "27149  Comparison of Epidemiological Variations in CO...   \n",
       "...                                                  ...   \n",
       "67974  Knowledge, Attitude, and Practices of Healthca...   \n",
       "67975  Safety and Efficacy of Anti-Il6-Receptor Tocil...   \n",
       "67976  Functional imaging of head and neck tumors usi...   \n",
       "67977                Effectiveness of 3D virtual imaging   \n",
       "67978  A prospective evaluation of thallium-201 singl...   \n",
       "\n",
       "                                                abstract  judgement  \n",
       "id                                                                   \n",
       "27145  The objective of the paper is to analyse chang...          0  \n",
       "27146  Leukoerythroblastic reaction in a patient with...          0  \n",
       "27147  [15O]-water PET was performed on 12 patients w...          0  \n",
       "27148  We present a method that significantly improve...          0  \n",
       "27149  The objective of this study is to compare the ...          0  \n",
       "...                                                  ...        ...  \n",
       "67974  In the current outbreak of novel coronavirus (...          0  \n",
       "67975  BACKGROUND: As the novel SARS-CoV-2 pandemic o...          0  \n",
       "67976  Positron emission tomography (PET) is an imagi...          0  \n",
       "67977                Effectiveness of 3D virtual imaging          0  \n",
       "67978  PURPOSE: The follow-up of patients with malign...          0  \n",
       "\n",
       "[40834 rows x 3 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 40834\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546b37a181fc41b0928b9a396d496866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\masa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "result = trainer.predict(submit_dataset, ignore_keys=['loss', 'last_hidden_state', 'hidden_states', 'attentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 5.5383935, -5.2530723],\n",
       "       [ 3.457717 , -2.8394465]], dtype=float32), label_ids=None, metrics={'test_runtime': 0.7026, 'test_samples_per_second': 2.846, 'test_steps_per_second': 1.423})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [71], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m softmax_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m softmax_output \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\masa\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32md:\\masa\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1198\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1198\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msoftmax(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim, _stacklevel\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32md:\\masa\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1512\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1510\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1511\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1512\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1513\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1514\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.5383935, -5.2530723],\n",
       "       [ 3.457717 , -2.8394465]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './input/livedoor_data.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./input/livedoor_data.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# カテゴリーのID列を付与しておく\u001b[39;00m\n\u001b[0;32m      6\u001b[0m categories \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32md:\\masa\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py:187\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[0;32m    186\u001b[0m excs_to_catch \u001b[39m=\u001b[39m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mModuleNotFoundError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m)\n\u001b[1;32m--> 187\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    188\u001b[0m     filepath_or_buffer,\n\u001b[0;32m    189\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    190\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    191\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    192\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    193\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m     \u001b[39m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[39m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[39m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m         \u001b[39m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    201\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\masa\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    799\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    801\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './input/livedoor_data.pickle'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_pickle('./input/livedoor_data.pickle')\n",
    "# カテゴリーのID列を付与しておく\n",
    "categories = df['category'].unique().tolist()\n",
    "category2id = {cat: categories.index(cat) for cat in categories}\n",
    "df['category_id'] = df['category'].map(lambda x: category2id[x])\n",
    "\n",
    "df.sample(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c5e9d213741e57af941aa78a333f873168089e7713d62d465ad09d7150e1980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
